{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "## Задание №1:\n",
    "1. Скачать минимум 100 текстовых страниц с помощью краулера.\n",
    "2. Записать каждую страницу в отдельный текстовый файл.\n",
    "3. Создать файл index.txt в котором хранится номер документа и ссылка на страницу.\n",
    "Входным аргументом программы должен быть веб-адрес страницы.\n",
    "Ссылки с первой страницы ведут на другие, которые также скачиваются и заносятся в файл index.txt. Если на первой странице не набралось достаточного количества страниц (100), то операция повторяется для дочерних страниц первой.\n",
    "Каждая страница должна содержать не менее 1000 слов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, glob\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html2text\n",
    "\n",
    "def getDomain(url):\n",
    "    domain='https://'+ urlparse(url).hostname\n",
    "    return domain\n",
    "\n",
    "def createPageTextFile(fileName, text):\n",
    "    global folder\n",
    "    file = open(\"{0}\\{1}.txt\".format(folder,fileName),\"x\",encoding=\"utf-8\")\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    \n",
    "def getPageUrls(url):\n",
    "    page=requests.get(url)\n",
    "    soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "    domain=getDomain(url)\n",
    "    items=soup.find_all('a')\n",
    "    urls=[]\n",
    "    \n",
    "    for item in items:\n",
    "        if item is None:\n",
    "            continue\n",
    "        url=item.get('href')\n",
    "        if url is None:\n",
    "            continue    \n",
    "        if url.startswith('/'):\n",
    "            urls.append(domain+url)\n",
    "        elif url.startswith('http'):\n",
    "            urls.append(url) \n",
    "            \n",
    "    return list(dict.fromkeys(urls))\n",
    "\n",
    "def getPageText(soup):\n",
    "    h=html2text.HTML2Text()\n",
    "    h.ignore_link=True\n",
    "    h.ignore_image=True\n",
    "    h.escape_snob=True\n",
    "    h.skip_internal_link=True\n",
    "    return h.handle(soup.prettify())\n",
    "\n",
    "def pageScraping(url,file_ind,url_ind):\n",
    "    global childPageUrls,f\n",
    "    urls=getPageUrls(url)\n",
    "##############################################\n",
    "    for element in urls:\n",
    "        if file_ind>100:\n",
    "            break\n",
    "        page=requests.get(element)\n",
    "        soup=BeautifulSoup(page.text,\"html.parser\")\n",
    "        text=getPageText(soup)\n",
    "        if len(text.split())<1000:\n",
    "            continue\n",
    "        createPageTextFile(file_ind,text)\n",
    "        f.write('{0} {1}\\n'.format(file_ind,element))\n",
    "        file_ind+=1\n",
    "        \n",
    "    if file_ind<=100:\n",
    "        pageScraping(childPageUrls[url_ind+1],file_ind,url_ind+1)\n",
    "        \n",
    "        \n",
    "        \n",
    "try:\n",
    "    site=\"https://www.microsoft.com/ru-ru\"\n",
    "    #site=\"https://www.litres.ru\"\n",
    "    childPageUrls=getPageUrls(site)\n",
    "    \n",
    "    folder=r'...\\IS\\Folder'\n",
    "    files=glob.glob(folder+'\\*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    f=open(r'...\\IS\\index.txt','w',encoding=\"utf-8\")\n",
    "    pageScraping(site,1,-1)\n",
    "    f.close()\n",
    "    print('Done!')\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "    \n",
    "    #другой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "1. Из сохраненных документов выделить отдельные слова (токенизация).\n",
    "2. Лемматизировать токены (допускается использование сторонних библиотек, устно могу спросить какие есть способы для лемматизации).\n",
    "Русский язык обязателен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os, glob\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "\n",
    "filesFolder=r'...\\IS\\Folder'\n",
    "lemmasFolder=r'...\\IS\\Lemmas'\n",
    "\n",
    "stop=['-','--','и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'чтоб', 'без', 'будто', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "\n",
    "try:\n",
    "    files=glob.glob(lemmasFolder+'\\*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    for i in range(1,101):\n",
    "        f=open('{0}\\{1}.txt'.format(filesFolder,i),\"r\",encoding=\"utf-8\")\n",
    "        text=f.read()\n",
    "        text=re.sub(r\"[^a-zA-Zа-яёА-ЯЁ0-9-]+\",' ',text)\n",
    "        words=word_tokenize(text.lower())\n",
    "        words=[word for word in words if word not in stop]\n",
    "        terms=[morph.parse(word)[0].normal_form for word in words]\n",
    "        file=open(\"{0}\\{1}.txt\".format(lemmasFolder,i),\"x\",encoding=\"utf-8\")\n",
    "        file.write('\\n'.join(terms))\n",
    "        file.close()\n",
    "    print('Done!')\n",
    "    \n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "\n",
    "1. Создать инвертированный список терминов (индекс).\n",
    "2. Реализовать булев поиск по построенному индексу (т.е. вводится выражение содержащее слова с тремя логическими И, ИЛИ, НЕ, по которому выдается список документов, содержащий данное выражение).\n",
    "\n",
    "Примеры:\n",
    "str1 & str2 | str3    str1 & !str2 | !str3\n",
    "str1 | str2 | str3    str1 | !str2 | !str3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "folder=r'...\\IS'\n",
    "lemmasFolder=r'...\\IS\\Lemmas'\n",
    "\n",
    "def createIndex(lemmas,folder):\n",
    "    indf=open('{0}\\windex.txt'.format(folder),\"w\",encoding='utf-8')\n",
    "    \n",
    "    \n",
    "    key=lemmas[0].split()[0]\n",
    "    docs=[lemmas[0].split()[1]]\n",
    "    for i in range(1,len(lemmas)):\n",
    "        elem=lemmas[i].split()\n",
    "        if elem[0]==key:\n",
    "            docs.append(elem[1])\n",
    "        else:\n",
    "            indf.write('{0} {1}\\n'.format(key,','.join(docs)))\n",
    "            key=elem[0]\n",
    "            docs=[elem[1]]\n",
    "    indf.write('{0} {1}\\n'.format(key,','.join(docs)))\n",
    "    indf.close()\n",
    "    \n",
    "def createLemmasDct(lemmasFolder):\n",
    "    lemmas=[]\n",
    "    for i in range(1,101):\n",
    "        f=open('{0}\\{1}.txt'.format(lemmasFolder,i),'r',encoding='utf-8')\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            lemmas.append('{0} {1}'.format(line,i))\n",
    "    lemmas=list(dict.fromkeys(lemmas))\n",
    "    lemmas=sorted(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "try:\n",
    "    lemmas=createLemmasDct(lemmasFolder)\n",
    "    createIndex(lemmas,folder)\n",
    "    print('Done!')\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка. Примеры:str1 & str2 | str3, str1 & !str2 | !str3, str1 | str2 | str3, str1 | !str2 | !str3 \n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "def splitDocsIndex(word,dct):\n",
    "    if word in dct.keys():\n",
    "        return dct[word].split(',')\n",
    "    return []\n",
    "\n",
    "def lemmatize(words):\n",
    "    morph=pymorphy2.MorphAnalyzer()\n",
    "    terms=[morph.parse(word)[0].normal_form for word in words]\n",
    "    return terms\n",
    "\n",
    "def getInd(i):\n",
    "    return int(i/2)\n",
    "\n",
    "def mergeLists(list1,list2):\n",
    "    return [doc for doc in list1 if doc in list2]\n",
    "\n",
    "def getTermDict():\n",
    "    global folder\n",
    "    f=open('{0}\\windex.txt'.format(folder),\"r\",encoding='utf-8')\n",
    "    lines=f.readlines()\n",
    "    termDict={line.split()[0]: line.split()[1] for line in lines}\n",
    "    return termDict\n",
    "\n",
    "def getPhraseDocs(phrase,termDict):\n",
    "    global not_s\n",
    "    phraseDocs=[]\n",
    "    for i in range(0,5,2):\n",
    "        if phrase[i][0]==not_s:\n",
    "            wordDoc=splitDocsIndex(phrase[i].split(not_s)[1],termDict)\n",
    "            phraseDocs.append([doc for doc in allDocs if doc not in wordDoc])\n",
    "        else:\n",
    "            phraseDocs.append(splitDocsIndex(phrase[i],termDict))\n",
    "    return phraseDocs\n",
    "\n",
    "def mergePhraseDocs(phrase,phraseDocs):\n",
    "    global and_s\n",
    "    if and_s in phrase:\n",
    "        ind=phrase.index(and_s)\n",
    "        list1=phraseDocs[getInd(ind-1)]\n",
    "        list2=phraseDocs[getInd(ind+1)]\n",
    "        mergedList=mergeLists(list1,list2)\n",
    "        if ind==1:\n",
    "            nextInd=3\n",
    "            reqInd=4\n",
    "        else:\n",
    "            nextInd=1\n",
    "            reqInd=0\n",
    "        if phrase[nextInd]==and_s:\n",
    "            resultList=mergeLists(mergedList,phraseDocs[getInd(reqInd)])\n",
    "        else:\n",
    "            resultList=mergedList+phraseDocs[getInd(reqInd)]\n",
    "    else:\n",
    "        mergedList=phraseDocs[0]+phraseDocs[1]\n",
    "        resultList=mergedList+phraseDocs[2]\n",
    "    return list(dict.fromkeys(resultList))\n",
    "\n",
    "\n",
    "request='погода | лес | !спит'\n",
    "request='погода & лес & !спит'\n",
    "\n",
    "and_s='&'\n",
    "or_s='|'\n",
    "not_s='!'\n",
    "folder=r'...\\IS'\n",
    "allDocs=['{0}'.format(i) for i in range(1,101)]\n",
    "\n",
    "\n",
    "try:\n",
    "    phrase=request.split()\n",
    "    if len(phrase)<5 or phrase[1] not in [and_s,or_s] or phrase[3] not in [and_s,or_s]:\n",
    "        raise Exception('Ошибка. Примеры:str1 & str2 | str3, str1 & !str2 | !str3, str1 | str2 | str3, str1 | !str2 | !str3 ')\n",
    "    \n",
    "    phrase=lemmatize(phrase)\n",
    "    termDict=getTermDict()\n",
    "\n",
    "    phraseDocs=getPhraseDocs(phrase,termDict)\n",
    "    result=mergePhraseDocs(phrase,phraseDocs)\n",
    "    result.sort(key=int)\n",
    "    print('Документы удовлетворяют условию \\'{0}\\': {1}'.format(request,', '.join(result)))\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "\n",
    "1. Подсчитать tf каждого термина.\n",
    "2. Подсчитать idf.\n",
    "3. Подсчитать tf-idf.\n",
    "В выводимых таблицах округлять значение до 3-5 знаков после запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import collections\n",
    "import math\n",
    "\n",
    "def termsIDF(folder,docsCount):\n",
    "    indxf=open('{0}\\windex.txt'.format(folder),\"r\",encoding='utf-8')\n",
    "    lines=indxf.readlines()\n",
    "    \n",
    "    idf_terms={}\n",
    "    for line in lines: \n",
    "        split=line.split()\n",
    "        key=split[0]\n",
    "        docs=split[1].split(',')\n",
    "        idf_terms[key]=round(math.log10(docsCount/len(docs)),5)\n",
    "        \n",
    "    return idf_terms\n",
    "\n",
    "def dfLine(i,term, tf, idf):\n",
    "    return {'Document': i, 'Term': term, 'tf': tf, 'idf': idf, 'tf-idf': round(tf * idf,5)}\n",
    "\n",
    "def docTermsTF(docName,terms):\n",
    "    global lemmasFolder\n",
    "    \n",
    "    tf_terms={}\n",
    "    f=open('{0}\\{1}.txt'.format(lemmasFolder,docName), \"r\", encoding='utf-8')\n",
    "    docTerms=f.readlines()\n",
    "    docTerms=[term.split('\\n')[0] for term in docTerms]\n",
    "    termsCount = len(docTerms)\n",
    "    termsCounter=collections.Counter(docTerms)\n",
    "    for term in terms:\n",
    "        tf_terms[term]=round(termsCounter[term]/termsCount,5)\n",
    "    return tf_terms\n",
    "\n",
    "def docDataFrame(i):\n",
    "    global terms,idf_terms\n",
    "    tf_terms=docTermsTF(i,terms)\n",
    "    df=pd.DataFrame(columns=['Document','Term','tf','idf','tf-idf'])\n",
    "    for term in tf_terms:\n",
    "        tf=tf_terms[term]\n",
    "        idf=idf_terms[term]\n",
    "        df=df.append(dfLine(i,term,tf,idf),ignore_index=True)\n",
    "    return df\n",
    "\n",
    "folder = r'...\\IS'\n",
    "lemmasFolder=r'...\\IS\\Lemmas'\n",
    "\n",
    "tdf=pd.DataFrame(columns=['Document','Term','tf','idf','tf-idf'])\n",
    "idf_terms=termsIDF(folder,100)\n",
    "indxf=open('{0}\\windex.txt'.format(folder),\"r\",encoding='utf-8')\n",
    "lines=indxf.readlines()\n",
    "terms=[line.split()[0] for line in lines]\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    doc=[i for i in range(1,101)]\n",
    "    with Pool(4) as pool:\n",
    "        res=pool.map(docDataFrame,doc)\n",
    "        tdf=tdf.append(res,ignore_index=True)\n",
    "    try:\n",
    "        tdf.to_excel('tdf.xlsx')\n",
    "    except ValueError as error:\n",
    "        part1=int(len(tdf)/2)\n",
    "        tdf.head(part1).to_excel('{0}\\\\tdf1.xlsx'.format(folder))\n",
    "        tdf.tail(len(tdf) - part1).to_excel('{0}\\\\tdf2.xlsx'.format(folder))\n",
    "    print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Document</th>\n",
       "      <th>Term</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310591</th>\n",
       "      <td>310591</td>\n",
       "      <td>22</td>\n",
       "      <td>drm</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544002</th>\n",
       "      <td>544002</td>\n",
       "      <td>38</td>\n",
       "      <td>expertclubmicrosoft</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.69897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338427</th>\n",
       "      <td>338427</td>\n",
       "      <td>24</td>\n",
       "      <td>bwq5fc9wdj6h</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.69897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130006</th>\n",
       "      <td>130006</td>\n",
       "      <td>9</td>\n",
       "      <td>процесс</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35655</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410221</th>\n",
       "      <td>410221</td>\n",
       "      <td>29</td>\n",
       "      <td>addressing</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29570</th>\n",
       "      <td>29570</td>\n",
       "      <td>3</td>\n",
       "      <td>20reference</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.69897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535101</th>\n",
       "      <td>535101</td>\n",
       "      <td>37</td>\n",
       "      <td>ww-registration-azure-hybrid-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.69897</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155745</th>\n",
       "      <td>155745</td>\n",
       "      <td>11</td>\n",
       "      <td>together</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92082</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415103</th>\n",
       "      <td>415103</td>\n",
       "      <td>29</td>\n",
       "      <td>myxbox</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Document                           Term   tf      idf  \\\n",
       "310591      310591        22                            drm  0.0  2.00000   \n",
       "544002      544002        38            expertclubmicrosoft  0.0  1.69897   \n",
       "338427      338427        24                   bwq5fc9wdj6h  0.0  1.69897   \n",
       "130006      130006         9                        процесс  0.0  0.35655   \n",
       "410221      410221        29                     addressing  0.0  2.00000   \n",
       "29570        29570         3                    20reference  0.0  1.69897   \n",
       "535101      535101        37  ww-registration-azure-hybrid-  0.0  1.69897   \n",
       "155745      155745        11                       together  0.0  0.92082   \n",
       "415103      415103        29                         myxbox  0.0  2.00000   \n",
       "\n",
       "        tf-idf  \n",
       "310591     0.0  \n",
       "544002     0.0  \n",
       "338427     0.0  \n",
       "130006     0.0  \n",
       "410221     0.0  \n",
       "29570      0.0  \n",
       "535101     0.0  \n",
       "155745     0.0  \n",
       "415103     0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "term=pd.read_excel('tdf1.xlsx',engine='openpyxl')\n",
    "term.sample(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5\n",
    "\n",
    "1. Разработать поисковую систему на основе векторного поиска по построенному индексу.\n",
    "\n",
    "Примеры запуска:\n",
    "1. word1 \n",
    "2. word1 word2\n",
    "3. word1 word2 word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "79 https://www.microsoft.com/licensing/default 0.06240038222273482\n",
      "22 https://www.microsoft.com/licensing/cloud-platform/windows-server 0.06239066382201604\n",
      "58 https://www.microsoft.com/microsoft-365/fwlink/?LinkID=206977 0.0306067678616649\n",
      "1 https://www.microsoft.com/microsoft-365/microsoft-office 0.030526784361693458\n",
      "74 https://www.microsoft.com/ru-ru/store/b/business?icid=CNavBusinessStore/edge?form=MI13F3&OCID=MI13F3 0.02599644624122572\n",
      "17 https://www.microsoft.com/ru-ru/store/b/business?icid=CNavBusinessStore 0.025926454745759706\n",
      "95 https://www.microsoft.com/ru-ru/education/products/office/default.aspx 0.02169334961241924\n",
      "38 https://www.microsoft.com/ru-ru/education/products/office/windows-10-apps 0.02169334961241924\n",
      "100 https://www.microsoft.com/enterprise/government 0.012250731526532721\n",
      "43 https://www.microsoft.com/enterprise 0.012245655379996879\n",
      "71 https://www.microsoft.com/microsoft-365/business/windows?icid=CNavGamesWindowsGames 0.0043721359564300065\n",
      "14 https://www.microsoft.com/microsoft-365/business/all-business 0.0043721359564300065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "\n",
    "def lemmatize(words):\n",
    "    morph=pymorphy2.MorphAnalyzer()\n",
    "    terms=[morph.parse(word)[0].normal_form for word in words]\n",
    "    return terms\n",
    "\n",
    "def getRequestVector(lines,request):\n",
    "    reqv=[0]*len(lines)\n",
    "    for i in range(len(lines)):\n",
    "        value=lines[i].split()[0]\n",
    "        if value in request:\n",
    "            reqv[i]=1\n",
    "            request.remove(value)\n",
    "    return reqv\n",
    "\n",
    "def getCosangles(reqv,vectors):\n",
    "    cosangles=[]\n",
    "    for i in range(1,101):\n",
    "        cosangle=np.dot(reqv,vectors[i]) / (np.linalg.norm(reqv)*np.linalg.norm(vectors[i]))\n",
    "        cosangles.append(cosangle)\n",
    "    return cosangles\n",
    "\n",
    "def getDocuments(cosangles):\n",
    "    docs={}\n",
    "    for i in range(len(cosangles)):\n",
    "        if cosangles[i] != 0:\n",
    "            docs[i+1]=cosangles[i]\n",
    "    docs={k: v for k, v in sorted(docs.items(),key=lambda item: item[1])}\n",
    "    return docs\n",
    "\n",
    "def getAllVectors(folder):\n",
    "    df1=pd.read_excel(r'{0}\\\\tdf1.xlsx'.format(folder))\n",
    "    df2=pd.read_excel(r'{0}\\\\tdf2.xlsx'.format(folder))\n",
    "    df=df1.append(df2,ignore_index=True)\n",
    "    vectors=df.groupby('Document')['tf-idf'].apply(list)\n",
    "    return vectors\n",
    "\n",
    "folder=r'...\\IS'\n",
    "indxf=open('{0}\\windex.txt'.format(folder),'r',encoding='utf-8')\n",
    "lines=indxf.readlines()\n",
    "vectors=getAllVectors(folder)\n",
    "\n",
    "request='покупать лицензии'\n",
    "req=lemmatize(request.split())\n",
    "reqv=getRequestVector(lines,req)\n",
    "\n",
    "cosangles=getCosangles(reqv,vectors)\n",
    "docs=getDocuments(cosangles)\n",
    "\n",
    "indexf=open('{0}\\index.txt'.format(folder), 'r', encoding='utf-8')\n",
    "index=indexf.readlines()\n",
    "\n",
    "documents=list(reversed(list(docs)))\n",
    "\n",
    "print('Result:')\n",
    "for i in documents:\n",
    "    print(i,index[i-1].split()[1],docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
